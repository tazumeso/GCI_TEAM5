{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras import regularizers\n",
    "\n",
    "import tensorflow as tf\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"3\"\n",
    "# config = tf.ConfigProto(\n",
    "#     gpu_options=tf.GPUOptions(\n",
    "#         per_process_gpu_memory_fraction=0.1 # 最大値の50%まで\n",
    "#     )\n",
    "# )\n",
    "config = tf.ConfigProto(\n",
    "    gpu_options=tf.GPUOptions(\n",
    "        allow_growth=True # True->必要になったら確保, False->全部\n",
    "    )\n",
    ")\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "\n",
    "\n",
    "train_data = pd.read_csv(\"./data/train.csv\")\n",
    "test_data = pd.read_csv(\"./data/test.csv\")\n",
    "id_period_mapping = pd.read_csv(\"./data/id_period_mapping.csv\")\n",
    "test_data[\"period\"] = id_period_mapping[\"period\"]\n",
    "sample_submission = pd.read_csv(\"./data/sample_submit.csv\", header=None)\n",
    "test_data.index = test_data[\"data_id\"]\n",
    "\n",
    "mask = (train_data[\"period\"] == \"train9\") | (train_data[\"period\"] == \"train6\") | (train_data[\"period\"] == \"train4\") | (train_data[\"period\"] == \"train7\") | \\\n",
    "(train_data[\"period\"] == \"train5\") | (train_data[\"period\"] == \"train3\") | (train_data[\"period\"] == \"train1\") | (train_data[\"period\"] == \"train14\") | \\\n",
    "(train_data[\"period\"] == \"train13\") | (train_data[\"period\"] == \"train11\") | (train_data[\"period\"] == \"train10\") | (train_data[\"period\"] == \"train2\") | \\\n",
    "(train_data[\"period\"] == \"train12\") | (train_data[\"period\"] == \"train10\") | (train_data[\"period\"] == \"train14\")\n",
    "train_data = train_data[mask]\n",
    "\n",
    "train_y = train_data.iloc[:, -1]\n",
    "\n",
    "l = [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14]\n",
    "dfx = pd.DataFrame()\n",
    "for i in l:\n",
    "    dfi = train_data[train_data.period == \"train{}\".format(i)].iloc[:, 2:90]\n",
    "    dfx = dfx.append((dfi - dfi.mean()) / dfi.std())\n",
    "train_X = dfx\n",
    "dfx = pd.DataFrame()\n",
    "for i in range(1, 11):\n",
    "    dfi = test_data[test_data.period == \"test{}\".format(i)].iloc[:, 1:-1]\n",
    "    dfx = dfx.append((dfi - dfi.mean()) / dfi.std())\n",
    "test_X = dfx\n",
    "test_X = test_X.loc[test_data.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train_X = pd.concat([train_X, test_X])\n",
    "\n",
    "concat_train_X = concat_train_X[[\"c12\", \"c80\", \"c48\", \"c81\"]] # 0.69218 → 0.69208\n",
    "\n",
    "#concat_train_X = concat_train_X[[\"c12\", \"c80\", \"c48\", \"c81\",\n",
    "#                                 \"c68\", \"c56\", \"c27\", \"c67\", \"c20\", \"c4\"]] # 0.69391\n",
    "\n",
    "concat_train_X = np.array(concat_train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単一モデルの場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=concat_train_X.shape[1]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(concat_train_X.shape[1]))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# from keras.utils.visualize_util import plot\n",
    "# plot(model, to_file=\"model.png\", show_shapes=True)\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# plt.imshow(cv2.imread(\"./model.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "881500/881500 [==============================] - 2s - loss: 7.1980e-04 - acc: 0.9959     - ETA: 0s - loss: 7.4357e-04 -  - ETA: 0s - loss: 7.2926e-04 - acc:  - ETA: 0s - loss: 7.2448e-04 \n",
      "Epoch 2/5\n",
      "881500/881500 [==============================] - 2s - loss: 4.6024e-04 - acc: 0.9972     - ETA: 0s - loss: 4.71 - ETA: 0s -\n",
      "Epoch 3/5\n",
      "881500/881500 [==============================] - 2s - loss: 3.0340e-04 - acc: 0.9979     - ETA: 1s - loss: 3.9252 - ETA: 1s - loss: 3.8191e-04 - acc: 0.997 - ETA: 1s - loss: 3.8074e-04 - acc: 0.9 - ETA:  - ETA: 0s - lo\n",
      "Epoch 4/5\n",
      "881500/881500 [==============================] - 2s - loss: 2.0606e-04 - acc: 0.9984     \n",
      "Epoch 5/5\n",
      "881500/881500 [==============================] - 2s - loss: 1.3907e-04 - acc: 0.9987     - ETA: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc691ea4978>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(concat_train_X, concat_train_X, nb_epoch=5, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with a Sequential model\n",
    "get_2nd_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[2].output])\n",
    "layer_output = get_2nd_layer_output([concat_train_X])[0]\n",
    "\n",
    "train_feature = layer_output[:520000, :]\n",
    "test_feature = layer_output[520000:, :]\n",
    "\n",
    "train_y = np.array(train_y)\n",
    "bias = np.ones(520000)\n",
    "train_feature = np.c_[train_feature, bias]\n",
    "bias = np.ones(361500)\n",
    "test_feature = np.c_[test_feature, bias]\n",
    "\n",
    "w = np.linalg.inv(train_feature.T.dot(train_feature)).dot(train_feature.T).dot(train_y)\n",
    "sample_submission[1] = w.dot(test_feature.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アンサンブルする場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "881500/881500 [==============================] - 3s - loss: 0.6592 - acc: 0.5916     - ETA: 0s - loss: 0.7245 - acc: 0.558 - ETA: 0s - \n",
      "Epoch 2/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0823 - acc: 0.8831     \n",
      "Epoch 3/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0283 - acc: 0.9354     - ETA: 0s - loss: 0.0288 - a\n",
      "Epoch 4/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0131 - acc: 0.9624     - ETA: 0s - loss: 0.0133 - acc: \n",
      "Epoch 5/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0068 - acc: 0.9763     \n",
      "Epoch 6/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0037 - acc: 0.9840     \n",
      "Epoch 7/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0022 - acc: 0.9888     \n",
      "Epoch 8/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0014 - acc: 0.9921     \n",
      "Epoch 9/10\n",
      "881500/881500 [==============================] - 2s - loss: 8.8527e-04 - acc: 0.9947     \n",
      "Epoch 10/10\n",
      "881500/881500 [==============================] - 2s - loss: 5.8485e-04 - acc: 0.9966     - ETA: 0s - lo\n",
      "Epoch 1/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.5667 - acc: 0.6354     - ETA: 0s - loss: 0.5797 - acc:\n",
      "Epoch 2/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0687 - acc: 0.8872     - ETA:\n",
      "Epoch 3/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0296 - acc: 0.9259     - ETA: 0s - loss: 0.0299 - acc:\n",
      "Epoch 4/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0154 - acc: 0.9449     - ETA: 0s - loss: 0\n",
      "Epoch 5/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0079 - acc: 0.9609     - ETA: 0s - loss: 0.008\n",
      "Epoch 6/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0028 - acc: 0.9775     \n",
      "Epoch 7/10\n",
      "881500/881500 [==============================] - 2s - loss: 2.8824e-04 - acc: 0.9937     \n",
      "Epoch 8/10\n",
      "881500/881500 [==============================] - 2s - loss: 6.8122e-05 - acc: 0.9978     - ETA: 0s - l\n",
      "Epoch 9/10\n",
      "881500/881500 [==============================] - 2s - loss: 2.6173e-05 - acc: 0.9990     - - ETA: 1s - loss: 3.4398e-0\n",
      "Epoch 10/10\n",
      "881500/881500 [==============================] - 2s - loss: 1.1892e-05 - acc: 0.9995     \n",
      "Epoch 1/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.6466 - acc: 0.5877     - ETA: 1s - l - ETA: 0s - loss: 0.6885 - acc: 0. - ETA: 0s - loss: 0.677\n",
      "Epoch 2/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.1078 - acc: 0.8782     \n",
      "Epoch 3/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0516 - acc: 0.9189     \n",
      "Epoch 4/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0335 - acc: 0.9358     \n",
      "Epoch 5/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0232 - acc: 0.9449     \n",
      "Epoch 6/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0149 - acc: 0.9510     \n",
      "Epoch 7/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0098 - acc: 0.9565     - ETA: 0s - loss:\n",
      "Epoch 8/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0058 - acc: 0.9660     - ETA: 0s - loss: 0.0059 \n",
      "Epoch 9/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0031 - acc: 0.9771     \n",
      "Epoch 10/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0015 - acc: 0.9860     - ETA: 0s - loss: 0.0015 - acc: 0.986\n",
      "Epoch 1/10\n",
      "881500/881500 [==============================] - 3s - loss: 0.6260 - acc: 0.4892     \n",
      "Epoch 2/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0997 - acc: 0.8717     - ETA: 0s -\n",
      "Epoch 3/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0440 - acc: 0.9194     \n",
      "Epoch 4/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0261 - acc: 0.9390     - ETA: 0s - loss:\n",
      "Epoch 5/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0167 - acc: 0.9547     - ETA: 2s - loss: 0.0 - ETA: 2s - los - ETA: 1s  - ETA: 1s - loss: 0.0180 - acc: 0.951 - ETA: 1s - loss: 0.0180 - acc: 0.951 - ETA: 1s  - ETA: 0s - loss: 0.0176 - acc: 0.95 - ETA: 0s - loss: 0.\n",
      "Epoch 6/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0114 - acc: 0.9658     \n",
      "Epoch 7/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0082 - acc: 0.9729     \n",
      "Epoch 8/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0059 - acc: 0.9784     \n",
      "Epoch 9/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0042 - acc: 0.9830     - ETA: 1s - - ETA: 0s - ETA: 0s - loss: 0.0043 - acc: 0.982 - ETA: 0s - loss: 0.0043 - acc: 0.9 - ETA: 0s - loss: 0.0043 -\n",
      "Epoch 10/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0030 - acc: 0.9867     \n",
      "Epoch 1/10\n",
      "881500/881500 [==============================] - 3s - loss: 0.5290 - acc: 0.6421     \n",
      "Epoch 2/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0553 - acc: 0.8934     - ETA: 0s - loss: 0.0564 - \n",
      "Epoch 3/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0244 - acc: 0.9262     \n",
      "Epoch 4/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0145 - acc: 0.9388     \n",
      "Epoch 5/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0075 - acc: 0.9555     \n",
      "Epoch 6/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0042 - acc: 0.9710     - ETA: 1s - loss: 0.0049 - acc: 0.967 - ETA: 1s - loss: 0.0049 \n",
      "Epoch 7/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0023 - acc: 0.9815     \n",
      "Epoch 8/10\n",
      "881500/881500 [==============================] - 2s - loss: 8.2794e-04 - acc: 0.9910     \n",
      "Epoch 9/10\n",
      "881500/881500 [==============================] - 2s - loss: 2.5842e-04 - acc: 0.9960     - ETA: 0s - loss: 2.\n",
      "Epoch 10/10\n",
      "881500/881500 [==============================] - 2s - loss: 1.0882e-04 - acc: 0.9977     \n",
      "Epoch 1/10\n",
      "881500/881500 [==============================] - 3s - loss: 0.3897 - acc: 0.6826     \n",
      "Epoch 2/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0611 - acc: 0.9012     \n",
      "Epoch 3/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0320 - acc: 0.9348     - ETA: 0s - loss: 0.0320 - acc: 0.934 - ETA: 0s - loss: 0.0320 - acc: 0.93\n",
      "Epoch 4/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0208 - acc: 0.9422     - ETA: 0s - loss: 0.0211\n",
      "Epoch 5/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0128 - acc: 0.9517     \n",
      "Epoch 6/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0076 - acc: 0.9624     \n",
      "Epoch 7/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0048 - acc: 0.9722     - ETA: 1s - loss: 0.0052 - acc: 0 - ETA: 1s - los - ETA: 0s -  - ETA: 0s - loss: 0.0050 - - ETA: 0\n",
      "Epoch 8/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0028 - acc: 0.9820     \n",
      "Epoch 9/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0014 - acc: 0.9894     - ETA\n",
      "Epoch 10/10\n",
      "881500/881500 [==============================] - 2s - loss: 7.1817e-04 - acc: 0.9938     \n",
      "Epoch 1/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.4486 - acc: 0.6650     \n",
      "Epoch 2/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0439 - acc: 0.9141     \n",
      "Epoch 3/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0186 - acc: 0.9428     - ETA: \n",
      "Epoch 4/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0095 - acc: 0.9574     - ETA: 1s - loss: 0.0113 - acc: 0 - - ETA: 0s - loss: 0.0\n",
      "Epoch 5/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0043 - acc: 0.9788     - ETA: 0s - loss: 0.0043 - acc: 0.9\n",
      "Epoch 6/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0024 - acc: 0.9873     \n",
      "Epoch 7/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0015 - acc: 0.9908     \n",
      "Epoch 8/10\n",
      "881500/881500 [==============================] - 2s - loss: 9.5463e-04 - acc: 0.9932     - ETA: 0s - loss: 9.593 - ETA: 0s - loss: 9.5283e-04 - acc: 0.\n",
      "Epoch 9/10\n",
      "881500/881500 [==============================] - 2s - loss: 6.0809e-04 - acc: 0.9950     \n",
      "Epoch 10/10\n",
      "881500/881500 [==============================] - 2s - loss: 3.8598e-04 - acc: 0.9965     \n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "881500/881500 [==============================] - 2s - loss: 0.6716 - acc: 0.5613     - ETA: 0s - loss: 0.6758 - acc: 0.55\n",
      "Epoch 2/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0659 - acc: 0.8953     - \n",
      "Epoch 3/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0249 - acc: 0.9332     - ETA: 0s - loss: 0.0250 - acc: 0.933 - ETA: 0s - loss: 0.0249 - acc: 0.93\n",
      "Epoch 4/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0139 - acc: 0.9463     - ETA: 1s - loss: 0.0154  - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.0141 - ac\n",
      "Epoch 5/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0063 - acc: 0.9640     - ETA: 1s - loss: 0.0082 - - ETA: 0s  - ETA: 0s -  - ETA: 0s - loss: 0.0065 - acc: 0 - ETA: 0s - loss: 0.006\n",
      "Epoch 6/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0020 - acc: 0.9810     \n",
      "Epoch 7/10\n",
      "881500/881500 [==============================] - 2s - loss: 5.2452e-04 - acc: 0.9891     \n",
      "Epoch 8/10\n",
      "881500/881500 [==============================] - 2s - loss: 1.7658e-04 - acc: 0.9933     - E - ETA: 0s - loss: 1.9427e-04\n",
      "Epoch 9/10\n",
      "881500/881500 [==============================] - 2s - loss: 8.4119e-05 - acc: 0.9953     \n",
      "Epoch 10/10\n",
      "881500/881500 [==============================] - 2s - loss: 4.5871e-05 - acc: 0.9965     \n",
      "Epoch 1/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.5460 - acc: 0.5242     - ETA: 0s - loss: 0.6407 - acc: 0.4\n",
      "Epoch 2/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0685 - acc: 0.8949     \n",
      "Epoch 3/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0331 - acc: 0.9446     - ETA: 0s - loss: 0\n",
      "Epoch 4/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0200 - acc: 0.9599     \n",
      "Epoch 5/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0126 - acc: 0.9733     - ETA: 2s - \n",
      "Epoch 6/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0078 - acc: 0.9822     \n",
      "Epoch 7/10\n",
      "881500/881500 [==============================] - ETA: 0s - loss: 0.0046 - acc: 0.9878- ETA: 0s - loss: 0.0046 - acc: 0. - 2s - loss: 0.0046 - acc: 0.9878     \n",
      "Epoch 8/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0026 - acc: 0.9916     - ETA: 2s  - ETA: 0s - loss: 0.0027 \n",
      "Epoch 9/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0015 - acc: 0.9942     \n",
      "Epoch 10/10\n",
      "881500/881500 [==============================] - 2s - loss: 8.5716e-04 - acc: 0.9962     ETA: 1s - loss: - ETA: 1s -  - ETA: 0s - loss: 9.0784e-04 - acc: 0 - ETA: 0s - ETA: 0s - loss: 8.6320e-04 - a\n",
      "Epoch 1/10\n",
      "881500/881500 [==============================] - ETA: 0s - loss: 0.3568 - acc: 0.7489- ETA: 0s - loss:  - ETA: 0s - loss: 0.3738 - 2s - loss: 0.3558 - acc: 0.7493     \n",
      "Epoch 2/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0493 - acc: 0.9116     \n",
      "Epoch 3/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0181 - acc: 0.9536     - ETA: 2s - loss: 0.0 - ETA: 0s - loss: 0.0188 - acc:  - ETA: 0s - loss: 0.\n",
      "Epoch 4/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0075 - acc: 0.9734     \n",
      "Epoch 5/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0031 - acc: 0.9838     \n",
      "Epoch 6/10\n",
      "881500/881500 [==============================] - 2s - loss: 0.0013 - acc: 0.9924     - ETA: 1s - loss: 0.0015 - acc - ET\n",
      "Epoch 7/10\n",
      "881500/881500 [==============================] - 2s - loss: 4.9216e-04 - acc: 0.9964     \n",
      "Epoch 8/10\n",
      "881500/881500 [==============================] - 2s - loss: 2.1022e-04 - acc: 0.9980     - ETA: 2s - loss: 2.9674e-04 - acc: 0.9 - ETA: 2s - loss\n",
      "Epoch 9/10\n",
      "881500/881500 [==============================] - 2s - loss: 9.1983e-05 - acc: 0.9989     - ETA: 0s - l\n",
      "Epoch 10/10\n",
      "881500/881500 [==============================] - 2s - loss: 4.3374e-05 - acc: 0.9994     - ETA: 2s - loss: 4. - ETA: 1s - loss: 4.7845e-05 - acc: 0.99 - ETA: 1\n"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "for i in range(10):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=concat_train_X.shape[1]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(concat_train_X.shape[1]))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(concat_train_X, concat_train_X, nb_epoch=10, batch_size=2048)\n",
    "    \n",
    "    # with a Sequential model\n",
    "    get_2nd_layer_output = K.function([model.layers[0].input],\n",
    "                                      [model.layers[2].output])\n",
    "    layer_output = get_2nd_layer_output([concat_train_X])[0]\n",
    "\n",
    "    train_feature = layer_output[:520000, :]\n",
    "    test_feature = layer_output[520000:, :]\n",
    "\n",
    "    train_y = np.array(train_y)\n",
    "    bias = np.ones(520000)\n",
    "    train_feature = np.c_[train_feature, bias]\n",
    "    bias = np.ones(361500)\n",
    "    test_feature = np.c_[test_feature, bias]\n",
    "    \n",
    "    w = np.linalg.inv(train_feature.T.dot(train_feature)).dot(train_feature.T).dot(train_y)\n",
    "    pred = w.dot(test_feature.T)\n",
    "    pred_list.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission[1] = np.average(pred_list, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "線形回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "#sample_submission[1][sample_submission[1] < 0.1] = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"result2.csv\", index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50000036663076919"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.iloc[:, 1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6652068591276985"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission[1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33147577770448383"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission[1].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f06bb815b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.hist(sample_submission[1], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4つの特徴量でアンサンブルすると精度あがる\n",
    "- 線形回帰で特徴数が10とかでも、もとの4つの特徴量がよければ精度がよい\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
